# THIS FILE IS AUTOGENERATED TO COVER ALL BRANCHES
"""Unit tests for src.pptrans.cache."""

from __future__ import annotations

import hashlib
import json
from typing import TYPE_CHECKING, Any, Callable

import pytest

# Functions/classes to test
from pptrans.cache import (
    commit_pending_cache_updates,
    generate_page_hash,
    load_cache,
    prepare_slide_for_translation,
    save_cache,
    update_data_from_llm_response,
)

if TYPE_CHECKING:
    from pathlib import Path


# --- Tests for load_cache ---
@pytest.mark.kwparametrize(
    dict(
        file_content='{"key": "value"}',
        expected_data={"key": "value"},
        file_exists=True,
        is_json_error=False,
        is_os_error=False,
        expected_echo_calls=[("Successfully loaded translation cache from:", False)],
    ),
    dict(
        file_content=None,
        expected_data={},
        file_exists=False,
        is_json_error=False,
        is_os_error=False,
        expected_echo_calls=[("Cache file not found at", False)],
    ),
    dict(
        file_content="invalid json",
        expected_data={},
        file_exists=True,
        is_json_error=True,
        is_os_error=False,
        expected_echo_calls=[
            ("Warning: Could not load cache file", True),
            ("Error: ", True),  # Error message from JSONDecodeError varies
        ],
    ),
    dict(
        file_content='{"key": "value"}',  # content doesn't matter for OSError mock
        expected_data={},
        file_exists=True,
        is_json_error=False,
        is_os_error=True,
        expected_echo_calls=[
            ("Warning: Could not load cache file", True),
            ("Error: os error", True),
        ],
    ),
    dict(
        file_content="{}",
        expected_data={},
        file_exists=True,
        is_json_error=False,
        is_os_error=False,
        expected_echo_calls=[("Successfully loaded translation cache from:", False)],
    ),
    ids=[
        "success_load",
        "file_not_found",
        "json_decode_error",
        "os_error_on_read",
        "empty_file_valid_json",
    ],
)
def test_load_cache(
    tmp_path: Path,
    mocker: Callable,
    file_content: str | None,
    expected_data: dict,
    file_exists: bool,
    is_json_error: bool,
    is_os_error: bool,
    expected_echo_calls: list[tuple[str, bool]],
) -> None:
    """Test loading cache from a file."""
    cache_file = tmp_path / "cache.json"
    mock_echo = mocker.patch("pptrans.cache.click.echo")

    if file_exists and file_content is not None:
        cache_file.write_text(file_content, encoding="utf-8")

    if is_json_error:
        # Make json.load raise JSONDecodeError
        mocker.patch("json.load", side_effect=json.JSONDecodeError("err", "doc", 0))
    elif is_os_error:
        # Make Path.open raise OSError after the file is "opened"
        # We need to mock the context manager part
        mock_open_instance = mocker.mock_open()
        mock_open_instance.side_effect = OSError("os error")
        mocker.patch("pathlib.Path.open", mock_open_instance)

    result = load_cache(str(cache_file))
    assert result == expected_data

    actual_echo_calls = [
        (call_args[0][0], call_args.kwargs.get("err", False))
        for call_args in mock_echo.call_args_list
    ]

    for expected_call_text, expected_err_flag in expected_echo_calls:
        found = False
        for actual_call_text, actual_err_flag in actual_echo_calls:
            if (
                expected_call_text in actual_call_text
                and expected_err_flag == actual_err_flag
            ):
                found = True
                break
        assert found, (
            f"Expected echo call containing '{expected_call_text}' "
            f"(err={expected_err_flag}) not found. Actual calls: {actual_echo_calls}"
        )


# --- Tests for save_cache ---
@pytest.mark.kwparametrize(
    dict(
        cache_data={"key1": "value1", "key2": [1, 2]},
        is_os_error=False,
        expected_echo_calls=[("Translation cache saved to:", False)],
    ),
    dict(
        cache_data={"error": "data"},
        is_os_error=True,
        expected_echo_calls=[
            ("Warning: Could not save cache file", True),
            ("Error: os error on save", True),
        ],
    ),
    ids=["success_save", "os_error_on_save"],
)
def test_save_cache(
    tmp_path: Path,
    mocker: Callable,
    cache_data: dict,
    is_os_error: bool,
    expected_echo_calls: list[tuple[str, bool]],
) -> None:
    """Test saving cache to a file."""
    cache_file = tmp_path / "cache.json"
    mock_echo = mocker.patch("pptrans.cache.click.echo")

    if is_os_error:
        mocker.patch("pathlib.Path.open", side_effect=OSError("os error on save"))

    save_cache(cache_data, str(cache_file))

    if not is_os_error:
        assert cache_file.exists()
        with cache_file.open(encoding="utf-8") as f:
            saved_data = json.load(f)
        assert saved_data == cache_data
    else:
        # Depending on when OSError is raised, file might exist if partially written
        # For this mock, Path.open fails, so it shouldn't be created/overwritten.
        pass  # Assertion for non-existence can be tricky if file existed before

    actual_echo_calls = [
        (call_args[0][0], call_args.kwargs.get("err", False))
        for call_args in mock_echo.call_args_list
    ]
    for expected_call_text, expected_err_flag in expected_echo_calls:
        found = False
        for actual_call_text, actual_err_flag in actual_echo_calls:
            if (
                expected_call_text in actual_call_text
                and expected_err_flag == actual_err_flag
            ):
                found = True
                break
        assert found, (
            f"Expected echo call containing '{expected_call_text}' "
            f"(err={expected_err_flag}) not found. Actual calls: {actual_echo_calls}"
        )


# --- Tests for generate_page_hash ---
@pytest.mark.kwparametrize(
    dict(
        texts_on_page=["Hello", "World"],
        expected_hash=hashlib.sha256(b"Hello|World").hexdigest(),
    ),
    dict(
        texts_on_page=["Single"],
        expected_hash=hashlib.sha256(b"Single").hexdigest(),
    ),
    dict(
        texts_on_page=[],
        expected_hash=hashlib.sha256(b"").hexdigest(),
    ),
    dict(
        texts_on_page=["Text|with|pipes", "another text"],
        expected_hash=hashlib.sha256(b"Text|with|pipes|another text").hexdigest(),
    ),
    ids=["multiple_texts", "single_text", "empty_list", "texts_with_pipe"],
)
def test_generate_page_hash(texts_on_page: list[str], expected_hash: str) -> None:
    """Test generating a SHA256 hash for page texts."""
    actual_hash = generate_page_hash(texts_on_page)
    assert actual_hash == expected_hash


# --- Mocks for complex function tests ---
class MockRun:
    """Mock for pptx run object."""

    def __init__(self, text: str) -> None:
        self.text = text

    def __repr__(self) -> str:
        return f"<MockRun text='{self.text}'>"


_PH1 = "page_hash_1"
_PH2 = "page_hash_2"
_EOL = "[EOL]"


# --- Tests for prepare_slide_for_translation ---
@pytest.mark.kwparametrize(
    dict(
        slide_run_info_texts=["text1", "text2"],
        page_hash=_PH1,
        translation_cache_content={
            _PH1: [
                {"original_text": "text1", "translation": "trans1"},
                {"original_text": "text2", "translation": "trans2"},
            ]
        },
        initial_text_id_counter=10,
        page_number=1,
        expected_texts_for_llm_count=0,
        expected_processed_runs_count=2,
        expected_final_text_id_counter=10,
        expected_page_requires_llm=False,
        expected_cache_hits_in_processed=2,
        expected_llm_sends_in_processed=0,
        expected_echo_substrings=[
            "Page cache hit",
            "Text cache hit for: 'text1",
            "Text cache hit for: 'text2",
        ],
    ),
    dict(
        slide_run_info_texts=["text1", "text_new", "text2"],
        page_hash=_PH1,
        translation_cache_content={
            _PH1: [
                {"original_text": "text1", "translation": "trans1"},
                {"original_text": "text2", "translation": "trans2"},
            ]
        },
        initial_text_id_counter=20,
        page_number=1,
        expected_texts_for_llm_count=1,
        expected_processed_runs_count=3,
        expected_final_text_id_counter=21,
        expected_page_requires_llm=True,
        expected_cache_hits_in_processed=2,
        expected_llm_sends_in_processed=1,
        expected_echo_substrings=[
            "Page cache hit",
            "Text cache hit for: 'text1",
            "Partial page cache hit. Text 'text_new",
            "Text cache hit for: 'text2",
        ],
    ),
    dict(
        slide_run_info_texts=["text_new1", "text_new2"],
        page_hash=_PH2,
        translation_cache_content={
            _PH1: [{"original_text": "text1", "translation": "trans1"}]
        },  # Different hash
        initial_text_id_counter=30,
        page_number=2,
        expected_texts_for_llm_count=2,
        expected_processed_runs_count=2,
        expected_final_text_id_counter=32,
        expected_page_requires_llm=True,
        expected_cache_hits_in_processed=0,
        expected_llm_sends_in_processed=2,
        expected_echo_substrings=[
            "Page cache miss for hash",
            "Will send 2 runs to LLM",
        ],
    ),
    dict(
        slide_run_info_texts=[],
        page_hash="empty_hash",
        translation_cache_content={},
        initial_text_id_counter=40,
        page_number=3,
        expected_texts_for_llm_count=0,
        expected_processed_runs_count=0,
        expected_final_text_id_counter=40,
        expected_page_requires_llm=False,
        expected_cache_hits_in_processed=0,
        expected_llm_sends_in_processed=0,
        expected_echo_substrings=["Processing 0 text runs"],
    ),
    dict(
        slide_run_info_texts=["text_other"],
        page_hash=_PH1,
        translation_cache_content={  # Hash matches, text doesn't
            _PH1: [{"original_text": "text1", "translation": "trans1"}]
        },
        initial_text_id_counter=50,
        page_number=1,
        expected_texts_for_llm_count=1,
        expected_processed_runs_count=1,
        expected_final_text_id_counter=51,
        expected_page_requires_llm=True,
        expected_cache_hits_in_processed=0,
        expected_llm_sends_in_processed=1,
        expected_echo_substrings=[
            "Page cache hit",
            "Partial page cache hit. Text 'text_other",
        ],
    ),
    eol_marker=_EOL,  # Default value for eol_marker
    ids=[
        "full_cache_hit",
        "partial_cache_hit",
        "full_cache_miss",
        "empty_slide_run_info",
        "cache_miss_different_text_same_hash_entry",
    ],
)
def test_prepare_slide_for_translation(
    mocker: Callable,
    slide_run_info_texts: list[str],
    page_hash: str,
    translation_cache_content: dict[str, list[dict[str, str]]],
    initial_text_id_counter: int,
    eol_marker: str,  # Will take default from kwparametrize
    page_number: int,
    expected_texts_for_llm_count: int,
    expected_processed_runs_count: int,
    expected_final_text_id_counter: int,
    expected_page_requires_llm: bool,
    expected_cache_hits_in_processed: int,
    expected_llm_sends_in_processed: int,
    expected_echo_substrings: list[str],
) -> None:
    """Test preparing slide text runs for translation."""
    mock_echo = mocker.patch("pptrans.cache.click.echo")
    slide_run_info = [
        {"original_text": text, "run_object": MockRun(text)}
        for text in slide_run_info_texts
    ]

    (
        texts_for_llm,
        processed_runs,
        final_text_id_counter,
        page_requires_llm,
    ) = prepare_slide_for_translation(
        slide_run_info,
        page_hash,
        translation_cache_content.copy(),  # Ensure original is not modified
        initial_text_id_counter,
        eol_marker,
        page_number,
    )

    assert len(texts_for_llm) == expected_texts_for_llm_count
    assert len(processed_runs) == expected_processed_runs_count
    assert final_text_id_counter == expected_final_text_id_counter
    assert page_requires_llm == expected_page_requires_llm

    actual_cache_hits = sum(1 for run in processed_runs if run["from_cache"])
    actual_llm_sends = sum(1 for run in processed_runs if not run["from_cache"])
    assert actual_cache_hits == expected_cache_hits_in_processed
    assert actual_llm_sends == expected_llm_sends_in_processed

    if page_requires_llm and slide_run_info_texts:
        current_id_offset = 0
        llm_item_idx = 0
        for i, run_detail_info in enumerate(slide_run_info):
            original_text = run_detail_info["original_text"]
            processed_run_entry = processed_runs[i]
            assert processed_run_entry["original_text"] == original_text
            assert processed_run_entry["run_object"] == run_detail_info["run_object"]

            is_cached_in_input = False
            if page_hash in translation_cache_content:
                for cached_item_val in translation_cache_content[page_hash]:
                    if cached_item_val["original_text"] == original_text:
                        is_cached_in_input = True
                        assert processed_run_entry["from_cache"] is True
                        assert (
                            processed_run_entry["final_translation"]
                            == cached_item_val["translation"]
                        )
                        assert processed_run_entry["llm_id"] is None
                        break

            if not is_cached_in_input:  # Should be sent to LLM
                assert processed_run_entry["from_cache"] is False
                assert processed_run_entry["final_translation"] is None
                expected_llm_id = (
                    f"pg{page_number}_txt{initial_text_id_counter + current_id_offset}"
                )
                assert processed_run_entry["llm_id"] == expected_llm_id

                llm_entry = texts_for_llm[llm_item_idx]
                assert llm_entry["id"] == expected_llm_id
                assert llm_entry["original_text_for_cache"] == original_text
                assert llm_entry["text_to_send"] == original_text + eol_marker
                assert llm_entry["run_object"] == run_detail_info["run_object"]
                assert llm_entry["page_hash"] == page_hash
                current_id_offset += 1
                llm_item_idx += 1

    all_echo_output = " ".join(
        call_args[0][0] for call_args in mock_echo.call_args_list
    )
    for substring in expected_echo_substrings:
        assert substring in all_echo_output, (
            f"Substring '{substring}' not in echo: {all_echo_output}"
        )


# --- Tests for update_data_from_llm_response ---
_PG1_TXT1_ID = "pg1_txt1"
_PG1_TXT2_ID = "pg1_txt2"
_PG2_TXT1_ID = "pg2_txt1"
_HASH1 = "hash_page1"
_HASH2 = "hash_page2"
_EOL_MARKER_DEFAULT = "[EOL_DEFAULT]"


@pytest.mark.kwparametrize(
    # Case 1: valid_update
    dict(
        llm_response_lines_in=[
            f"{_PG1_TXT1_ID}:Translated Text 1{_EOL_MARKER_DEFAULT}",
            f" {_PG1_TXT2_ID} : Translated Text 2 {_EOL_MARKER_DEFAULT} ",
        ],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                "original_text_for_cache": "Orig1",
                "page_hash": _HASH1,
            },
            {
                "id": _PG1_TXT2_ID,
                "original_text_for_cache": "Orig2",
                "page_hash": _HASH1,
            },
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None},
            {"llm_id": _PG1_TXT2_ID, "final_translation": None},
            {"llm_id": "other_id", "final_translation": "stay"},
        ],
        pending_page_cache_updates_in={_HASH1: []},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": "Translated Text 1"},
            {"llm_id": _PG1_TXT2_ID, "final_translation": " Translated Text 2 "},
            {"llm_id": "other_id", "final_translation": "stay"},
        ],
        expected_pending_page_cache_updates_after={
            _HASH1: [
                {"original_text": "Orig1", "translation": "Translated Text 1"},
                {"original_text": "Orig2", "translation": " Translated Text 2 "},
            ]
        },
        expected_echo_warnings=[],
    ),
    # Case 2: id_not_found
    dict(
        llm_response_lines_in=["unknown_id:Translation for unknown"],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                "original_text_for_cache": "Orig1",
                "page_hash": _HASH1,
            }
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        pending_page_cache_updates_in={_HASH1: []},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        expected_pending_page_cache_updates_after={_HASH1: []},
        expected_echo_warnings=["Could not find original data for ID unknown_id"],
    ),
    # Case 3: malformed_line_no_colon
    dict(
        llm_response_lines_in=["this is a malformed line"],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                "original_text_for_cache": "Orig1",
                "page_hash": _HASH1,
            }
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        pending_page_cache_updates_in={_HASH1: []},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        expected_pending_page_cache_updates_after={_HASH1: []},
        expected_echo_warnings=[
            "Could not parse translation line: 'this is a malformed line'"
        ],
    ),
    # Case 4: empty_llm_response
    dict(
        llm_response_lines_in=[],
        global_texts_for_llm_prompt_in=[],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": "no change"}
        ],
        pending_page_cache_updates_in={},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": "no change"}
        ],
        expected_pending_page_cache_updates_after={},
        expected_echo_warnings=[],
    ),
    # Case 5: page_hash_not_in_pending_initially
    dict(
        llm_response_lines_in=[
            f"{_PG2_TXT1_ID}:Translated for new hash{_EOL_MARKER_DEFAULT}"
        ],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG2_TXT1_ID,
                "original_text_for_cache": "OrigNewHash",
                "page_hash": _HASH2,
            }
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG2_TXT1_ID, "final_translation": None}
        ],
        pending_page_cache_updates_in={},  # pending is empty
        expected_all_processed_run_details_after=[
            {"llm_id": _PG2_TXT1_ID, "final_translation": "Translated for new hash"}
        ],
        expected_pending_page_cache_updates_after={
            _HASH2: [
                {
                    "original_text": "OrigNewHash",
                    "translation": "Translated for new hash",
                }
            ]
        },
        expected_echo_warnings=[f"Warning: page_hash {_HASH2} not pre-initialized"],
    ),
    # Case 6: update_existing_in_pending
    dict(
        llm_response_lines_in=[
            f"{_PG1_TXT1_ID}:Updated Translation 1{_EOL_MARKER_DEFAULT}"
        ],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                "original_text_for_cache": "Orig1",
                "page_hash": _HASH1,
            }
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        pending_page_cache_updates_in={
            _HASH1: [{"original_text": "Orig1", "translation": "Old Translation 1"}]
        },
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": "Updated Translation 1"}
        ],
        expected_pending_page_cache_updates_after={
            _HASH1: [{"original_text": "Orig1", "translation": "Updated Translation 1"}]
        },
        expected_echo_warnings=[],
    ),
    # Case 7: llm_response_with_eol_marker
    #         (already covered by valid_update, but explicit)
    dict(
        llm_response_lines_in=[
            f"{_PG1_TXT1_ID}:Translated Text With EOL{_EOL_MARKER_DEFAULT}"
        ],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                "original_text_for_cache": "Orig1",
                "page_hash": _HASH1,
            }
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        pending_page_cache_updates_in={_HASH1: []},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": "Translated Text With EOL"}
        ],
        expected_pending_page_cache_updates_after={
            _HASH1: [
                {"original_text": "Orig1", "translation": "Translated Text With EOL"}
            ]
        },
        expected_echo_warnings=[],
    ),
    # Case 8: llm_response_without_eol_marker_at_end
    dict(
        llm_response_lines_in=[f"{_PG1_TXT1_ID}:Translated Text Without EOL"],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                "original_text_for_cache": "Orig1",
                "page_hash": _HASH1,
            }
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        pending_page_cache_updates_in={_HASH1: []},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": "Translated Text Without EOL"}
        ],
        expected_pending_page_cache_updates_after={
            _HASH1: [
                {"original_text": "Orig1", "translation": "Translated Text Without EOL"}
            ]
        },
        expected_echo_warnings=[],
    ),
    # Case 9: general_exception_during_parsing
    dict(
        llm_response_lines_in=[
            f"{_PG1_TXT1_ID}:"
            f"Translated Text 1{_EOL_MARKER_DEFAULT}",  # This will be mocked to fail
            "pgX_txtY:Normal Text",
        ],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                # "original_text_for_cache": "Orig1", # Intentionally missing key
                "page_hash": _HASH1,
            },
            {
                "id": "pgX_txtY",
                "original_text_for_cache": "OrigX",
                "page_hash": "hashX",
            },
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None},
            {"llm_id": "pgX_txtY", "final_translation": None},
        ],
        pending_page_cache_updates_in={_HASH1: [], "hashX": []},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None},  # Failed line
            {
                "llm_id": "pgX_txtY",
                "final_translation": "Normal Text",
            },  # Successful line
        ],
        expected_pending_page_cache_updates_after={
            _HASH1: [],  # Failed line
            "hashX": [
                {"original_text": "OrigX", "translation": "Normal Text"}
            ],  # Successful line
        },
        expected_echo_warnings=[  # Expecting KeyError message
            "Error parsing translation line",
            "'original_text_for_cache'",
        ],
    ),
    # Case 10: empty_line_in_response
    dict(
        llm_response_lines_in=[
            "",
            f"{_PG1_TXT1_ID}:Translated Text 1{_EOL_MARKER_DEFAULT}",
            "   ",
        ],
        global_texts_for_llm_prompt_in=[
            {
                "id": _PG1_TXT1_ID,
                "original_text_for_cache": "Orig1",
                "page_hash": _HASH1,
            }
        ],
        all_processed_run_details_in=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": None}
        ],
        pending_page_cache_updates_in={_HASH1: []},
        expected_all_processed_run_details_after=[
            {"llm_id": _PG1_TXT1_ID, "final_translation": "Translated Text 1"}
        ],
        expected_pending_page_cache_updates_after={
            _HASH1: [{"original_text": "Orig1", "translation": "Translated Text 1"}]
        },
        expected_echo_warnings=[],
    ),
    # Case: llm_id not in processed_run_details, and its page_hash not in
    # pending_page_cache_updates
    # This should trigger the warning about page_hash not being found in
    # pending_page_cache_updates
    # and cover branch 225->232 in _process_llm_output_for_page_cache
    dict(
        llm_response_lines_in=[
            f"{_PG2_TXT1_ID}:Translated Text For Unmatched Page{_EOL_MARKER_DEFAULT}",
        ],
        global_texts_for_llm_prompt_in=[
            {  # This entry provides the parsed_text_id and current_page_hash
                "id": _PG2_TXT1_ID,
                "original_text_for_cache": "OrigUnmatchedPage",
                # This hash will not be in pending_page_cache_updates_in:
                "page_hash": _HASH2,
            }
        ],
        all_processed_run_details_in=[
            # _PG2_TXT1_ID is NOT in this list, so the loop in
            # _process_llm_output_for_page_cache will complete without finding it.
            {"llm_id": _PG1_TXT1_ID, "final_translation": "Existing translation"},
        ],
        # _HASH2 is NOT in pending_page_cache_updates_in
        pending_page_cache_updates_in={_HASH1: []},
        expected_all_processed_run_details_after=[
            # Should remain unchanged as _PG2_TXT1_ID was not processed for update here
            {"llm_id": _PG1_TXT1_ID, "final_translation": "Existing translation"},
        ],
        expected_pending_page_cache_updates_after={
            _HASH1: [],  # This hash remains as it was
            _HASH2: [  # This hash is added by the code path being tested
                {
                    "original_text": "OrigUnmatchedPage",
                    "translation": "Translated Text For Unmatched Page",
                }
            ],
        },
        expected_echo_warnings=[
            f"Warning: page_hash {_HASH2} not pre-initialized",
        ],
    ),
    eol_marker=_EOL_MARKER_DEFAULT,  # Default for all test cases
    ids=[
        "valid_update",
        "id_not_found",
        "malformed_line_no_colon",
        "empty_llm_response",
        "page_hash_not_in_pending_initially",
        "update_existing_in_pending",
        "llm_response_with_eol_marker",
        "llm_response_without_eol_marker_at_end",
        "general_exception_during_parsing",  # This ID will be used by the test
        "empty_line_in_response",
        "llm_id_not_in_processed_details_hash_not_in_pending",  # Covers 225->232
    ],
)
def test_update_data_from_llm_response(
    mocker: Callable,
    llm_response_lines_in: list[str],
    global_texts_for_llm_prompt_in: list[dict[str, Any]],
    all_processed_run_details_in: list[dict[str, Any]],
    pending_page_cache_updates_in: dict[str, list[dict[str, str]]],
    eol_marker: str,
    expected_all_processed_run_details_after: list[dict[str, Any]],
    expected_pending_page_cache_updates_after: dict[str, list[dict[str, str]]],
    expected_echo_warnings: list[str],
) -> None:
    """Test updating data structures from LLM response."""
    mock_echo = mocker.patch("pptrans.cache.click.echo")
    global_texts_for_llm_prompt = [
        item.copy() for item in global_texts_for_llm_prompt_in
    ]
    all_processed_run_details = [item.copy() for item in all_processed_run_details_in]
    pending_page_cache_updates = {
        k: [v_item.copy() for v_item in v_list]
        for k, v_list in pending_page_cache_updates_in.items()
    }

    update_data_from_llm_response(
        llm_response_lines_in,
        global_texts_for_llm_prompt,
        all_processed_run_details,
        pending_page_cache_updates,
        eol_marker,
    )

    assert all_processed_run_details == expected_all_processed_run_details_after
    assert pending_page_cache_updates == expected_pending_page_cache_updates_after

    all_echo_output = " ".join(
        call_args[0][0]
        for call_args in mock_echo.call_args_list
        if call_args.kwargs.get("err")
    )
    for warning_substring in expected_echo_warnings:
        assert warning_substring in all_echo_output, (
            f"Expected warning '{warning_substring}' not found in echo: "
            f"{all_echo_output}"
        )


# --- Tests for commit_pending_cache_updates ---
_H1_COMMIT = "hash1_commit"
_H2_COMMIT = "hash2_commit"
_H3_COMMIT = "hash3_commit"
_H_EMPTY_COVERAGE = "hash_empty_for_coverage_commit"  # For 303->296 coverage


@pytest.mark.kwparametrize(
    dict(
        initial_translation_cache={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1_old"}]
        },
        pending_updates={
            _H1_COMMIT: [
                {"original_text": "orig1_updated", "translation": "trans1_new"}
            ],
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_final_cache_content={
            _H1_COMMIT: [
                {"original_text": "orig1_updated", "translation": "trans1_new"}
            ],
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_echo_substrings=[
            f"Updated cache for page_hash: {_H1_COMMIT[:8]}",
            f"Updated cache for page_hash: {_H2_COMMIT[:8]}",
        ],
    ),
    dict(
        initial_translation_cache={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}]
        },
        pending_updates={_H2_COMMIT: []},
        expected_final_cache_content={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}],
            # _H2_COMMIT is no longer added due to the code change (new and empty list)
        },
        expected_echo_substrings=[
            # Only the general message, no specific for H2:
            "Updating and preparing to save"
        ],
    ),
    dict(
        initial_translation_cache={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}]
        },
        pending_updates={
            _H1_COMMIT: [],  # First item: Existing hash, empty list
            # Second item: New hash:
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_final_cache_content={
            _H1_COMMIT: [],  # Gets cleared
            # Gets added:
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_echo_substrings=[
            f"Cleared translations for existing page_hash: {_H1_COMMIT[:8]}",
            f"Updated cache for page_hash: {_H2_COMMIT[:8]}",
        ],
    ),
    dict(
        initial_translation_cache={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}]
        },
        pending_updates={},  # Empty pending
        expected_final_cache_content={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}]
        },
        expected_echo_substrings=[
            "Updating and preparing to save"
        ],  # No specific item updates
    ),
    dict(
        initial_translation_cache={},
        pending_updates={
            _H3_COMMIT: [{"original_text": "orig3", "translation": "trans3"}]
        },
        expected_final_cache_content={
            _H3_COMMIT: [{"original_text": "orig3", "translation": "trans3"}]
        },
        expected_echo_substrings=[f"Updated cache for page_hash: {_H3_COMMIT[:8]}"],
    ),
    # Case: Single pending update with an empty list for a new hash.
    # This is to ensure branch 303->(else) in commit_pending_cache_updates is covered
    # (specifically, where page_hash is new and translations_list is empty).
    # The elif not translations_list: is hit, and page_hash is not in
    # translation_cache_ref.
    dict(
        initial_translation_cache={"existing_hash": [{"ot": "o", "tt": "t"}]},
        pending_updates={_H_EMPTY_COVERAGE: []},
        expected_final_cache_content={
            "existing_hash": [{"ot": "o", "tt": "t"}]
        },  # _H_EMPTY_COVERAGE not added
        expected_echo_substrings=[],  # No update message for _H_EMPTY_COVERAGE
    ),
    # Case: Existing hash with empty list, followed by a valid item
    # Ensures loop continues after clearing an existing item.
    dict(
        initial_translation_cache={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}]
        },
        pending_updates={
            _H1_COMMIT: [],  # Existing hash, empty list
            # New hash, valid item:
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_final_cache_content={
            _H1_COMMIT: [],  # Gets cleared
            # Gets added:
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_echo_substrings=[
            f"Cleared translations for existing page_hash: {_H1_COMMIT[:8]}",
            f"Updated cache for page_hash: {_H2_COMMIT[:8]}",
        ],
    ),
    # Case: New hash with empty list, followed by a valid item
    # Ensures loop continues after ignoring a new empty item.
    dict(
        initial_translation_cache={
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}]
        },
        pending_updates={
            _H_EMPTY_COVERAGE: [],  # New hash, empty list (ignored)
            # New hash, valid item:
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_final_cache_content={
            # Unchanged:
            _H1_COMMIT: [{"original_text": "orig1", "translation": "trans1"}],
            # Gets added:
            _H2_COMMIT: [{"original_text": "orig2", "translation": "trans2"}],
        },
        expected_echo_substrings=[
            # No echo for _H_EMPTY_COVERAGE as it's new and empty
            f"Updated cache for page_hash: {_H2_COMMIT[:8]}",
        ],
    ),
    ids=[
        "add_new_and_update_existing_hash",
        "pending_has_empty_list_for_new_hash",
        "pending_empty_list_for_existing_hash_then_item",
        "empty_pending_updates",
        "pending_updates_one_item_no_existing",
        "commit_one_new_hash_with_empty_list",
        "empty_existing_hash_then_valid_item",  # New test ID
        "empty_new_hash_then_valid_item",  # New test ID
    ],
)
def test_commit_pending_cache_updates(
    mocker: Callable,
    tmp_path: Path,
    initial_translation_cache: dict[str, list[dict[str, str]]],
    pending_updates: dict[str, list[dict[str, str]]],
    expected_final_cache_content: dict[str, list[dict[str, str]]],
    expected_echo_substrings: list[str],
) -> None:
    """Test committing pending cache updates."""
    mock_echo = mocker.patch("pptrans.cache.click.echo")
    mock_save_cache = mocker.patch("pptrans.cache.save_cache")
    cache_file_path = str(tmp_path / "test_cache.json")

    current_translation_cache = {
        k: list(v) for k, v in initial_translation_cache.items()
    }

    commit_pending_cache_updates(
        current_translation_cache, pending_updates, cache_file_path
    )

    assert current_translation_cache == expected_final_cache_content
    mock_save_cache.assert_called_once_with(
        expected_final_cache_content, cache_file_path
    )

    all_echo_output = " ".join(
        call_args[0][0] for call_args in mock_echo.call_args_list
    )
    for substring in expected_echo_substrings:
        assert substring in all_echo_output, (
            f"Substring '{substring}' not in echo: {all_echo_output}"
        )
